#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
@file: llm_analyzer.py
@desc: LLM-based analysis for GitHub trending repositories
"""

import json
from typing import List, Dict, Optional
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential
import config


class LLMAnalyzer:
    """Analyzer using LLM to provide insights on trending repositories"""

    def __init__(self):
        """Initialize the LLM client"""
        self.client = OpenAI(
            api_key=config.LLM_API_KEY,
            base_url=config.LLM_BASE_URL
        )
        self.model = config.LLM_MODEL

    def _build_repos_summary(self, repos: List[Dict]) -> str:
        """Build a text summary of repositories for LLM input"""
        summary_parts = []
        for i, repo in enumerate(repos, 1):
            summary_parts.append(
                f"{i}. **{repo.get('full_name', 'Unknown')}** ({repo.get('language', 'Unknown')})\n"
                f"   - â­ Stars: {repo.get('stars', 0):,} (+{repo.get('stars_today', 0):,} today)\n"
                f"   - ðŸ´ Forks: {repo.get('forks', 0):,}\n"
                f"   - ðŸ“ Description: {repo.get('description', 'No description')}\n"
                f"   - ðŸ”— URL: {repo.get('url', '')}"
            )
        return "\n\n".join(summary_parts)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def analyze_trends(self, repos: List[Dict], analysis_type: str = "comprehensive") -> str:
        """
        Analyze trending repositories using LLM
        
        Args:
            repos: List of repository dictionaries
            analysis_type: Type of analysis - 'comprehensive', 'brief', 'technical'
        
        Returns:
            Analysis text from LLM
        """
        repos_summary = self._build_repos_summary(repos)
        
        prompts = {
            "comprehensive": """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æŠ€æœ¯åˆ†æžå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹ GitHub çƒ­é—¨é¡¹ç›®è¿›è¡Œå…¨é¢åˆ†æžï¼š

## ä»Šæ—¥ GitHub Trending é¡¹ç›®åˆ—è¡¨ï¼š

{repos}

è¯·æä¾›ä»¥ä¸‹åˆ†æžå†…å®¹ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰ï¼š

### 1. ðŸ“Š è¶‹åŠ¿æ¦‚è§ˆ
ç®€è¦æ€»ç»“ä»Šæ—¥çƒ­é—¨é¡¹ç›®çš„æ•´ä½“è¶‹åŠ¿ï¼ŒåŒ…æ‹¬ä¸»è¦æŠ€æœ¯æ–¹å‘å’Œçƒ­ç‚¹é¢†åŸŸã€‚

### 2. ðŸŒŸ é‡ç‚¹é¡¹ç›®æŽ¨èï¼ˆé€‰æ‹©3-5ä¸ªæœ€å€¼å¾—å…³æ³¨çš„é¡¹ç›®ï¼‰
å¯¹æ¯ä¸ªæŽ¨èé¡¹ç›®è¿›è¡Œè¯¦ç»†åˆ†æžï¼š
- é¡¹ç›®äº®ç‚¹å’Œåˆ›æ–°ç‚¹
- é€‚ç”¨åœºæ™¯å’Œç›®æ ‡ç”¨æˆ·
- æŠ€æœ¯æ ˆå’Œå®žçŽ°ç‰¹ç‚¹
- å­¦ä¹ ä»·å€¼å’Œå®žç”¨ä»·å€¼

### 3. ðŸ” æŠ€æœ¯æ´žå¯Ÿ
- ä»Žè¿™äº›é¡¹ç›®ä¸­è§‚å¯Ÿåˆ°çš„æŠ€æœ¯è¶‹åŠ¿
- å€¼å¾—å…³æ³¨çš„æ–°å…´æŠ€æœ¯æˆ–æ¡†æž¶
- å¼€å‘è€…ç¤¾åŒºçš„å…³æ³¨ç„¦ç‚¹

### 4. ðŸ’¡ å»ºè®®
- å¯¹å¼€å‘è€…çš„å­¦ä¹ å»ºè®®
- å“ªäº›é¡¹ç›®å€¼å¾—æ·±å…¥ç ”ç©¶
- æ½œåœ¨çš„åº”ç”¨æœºä¼š

è¯·ç¡®ä¿åˆ†æžå†…å®¹ä¸“ä¸šã€æœ‰æ·±åº¦ï¼Œå¯¹å¼€å‘è€…æœ‰å®žé™…å‚è€ƒä»·å€¼ã€‚""",

            "brief": """ä½ æ˜¯ä¸€ä½æŠ€æœ¯ç¼–è¾‘ï¼Œè¯·å¯¹ä»¥ä¸‹ GitHub çƒ­é—¨é¡¹ç›®è¿›è¡Œç®€è¦åˆ†æžï¼š

## ä»Šæ—¥ GitHub Trending é¡¹ç›®åˆ—è¡¨ï¼š

{repos}

è¯·ç”¨ä¸­æ–‡æä¾›ç®€æ´çš„åˆ†æžæ‘˜è¦ï¼ˆ300å­—ä»¥å†…ï¼‰ï¼ŒåŒ…æ‹¬ï¼š
1. ä»Šæ—¥ä¸»è¦æŠ€æœ¯è¶‹åŠ¿
2. 3ä¸ªæœ€å€¼å¾—å…³æ³¨çš„é¡¹ç›®åŠåŽŸå› 
3. ä¸€å¥è¯æ€»ç»“ä»Šæ—¥çƒ­ç‚¹""",

            "technical": """ä½ æ˜¯ä¸€ä½é«˜çº§è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹ GitHub çƒ­é—¨é¡¹ç›®è¿›è¡ŒæŠ€æœ¯å±‚é¢çš„æ·±åº¦åˆ†æžï¼š

## ä»Šæ—¥ GitHub Trending é¡¹ç›®åˆ—è¡¨ï¼š

{repos}

è¯·ç”¨ä¸­æ–‡æä¾›æŠ€æœ¯åˆ†æžï¼ŒåŒ…æ‹¬ï¼š

### 1. æŠ€æœ¯æ ˆåˆ†å¸ƒ
åˆ†æžä»Šæ—¥çƒ­é—¨é¡¹ç›®ä½¿ç”¨çš„ä¸»è¦æŠ€æœ¯æ ˆå’Œç¼–ç¨‹è¯­è¨€åˆ†å¸ƒã€‚

### 2. æž¶æž„ç‰¹ç‚¹
é€‰æ‹©2-3ä¸ªé¡¹ç›®åˆ†æžå…¶æž¶æž„è®¾è®¡å’ŒæŠ€æœ¯å®žçŽ°çš„äº®ç‚¹ã€‚

### 3. ä»£ç è´¨é‡æŒ‡æ ‡
åŸºäºŽ star/fork æ¯”ä¾‹ç­‰æ•°æ®åˆ†æžé¡¹ç›®çš„ç¤¾åŒºå‚ä¸Žåº¦å’Œä»£ç è´¨é‡ã€‚

### 4. æŠ€æœ¯å»ºè®®
å¯¹äºŽæƒ³è¦å­¦ä¹ æˆ–è´¡çŒ®è¿™äº›é¡¹ç›®çš„å¼€å‘è€…çš„æŠ€æœ¯å»ºè®®ã€‚"""
        }
        
        prompt = prompts.get(analysis_type, prompts["comprehensive"])
        formatted_prompt = prompt.format(repos=repos_summary)
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯åˆ†æžå¸ˆï¼Œæ“…é•¿åˆ†æžå¼€æºé¡¹ç›®å’ŒæŠ€æœ¯è¶‹åŠ¿ã€‚ä½ çš„åˆ†æžåº”è¯¥ä¸“ä¸šã€æœ‰æ·±åº¦ã€å¯¹å¼€å‘è€…æœ‰å®žé™…ä»·å€¼ã€‚"
                },
                {
                    "role": "user",
                    "content": formatted_prompt
                }
            ],
            temperature=0.7,
            max_tokens=4000
        )
        
        return response.choices[0].message.content

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def analyze_single_repo(self, repo: Dict) -> str:
        """
        Analyze a single repository in detail
        
        Args:
            repo: Repository dictionary
        
        Returns:
            Detailed analysis text
        """
        prompt = f"""è¯·å¯¹ä»¥ä¸‹ GitHub é¡¹ç›®è¿›è¡Œè¯¦ç»†åˆ†æžï¼š

é¡¹ç›®åç§°: {repo.get('full_name', 'Unknown')}
ç¼–ç¨‹è¯­è¨€: {repo.get('language', 'Unknown')}
Star æ•°: {repo.get('stars', 0):,}
ä»Šæ—¥æ–°å¢ž Star: {repo.get('stars_today', 0):,}
Fork æ•°: {repo.get('forks', 0):,}
é¡¹ç›®æè¿°: {repo.get('description', 'No description')}
é¡¹ç›®é“¾æŽ¥: {repo.get('url', '')}

è¯·ç”¨ä¸­æ–‡æä¾›ä»¥ä¸‹åˆ†æžï¼š
1. é¡¹ç›®å®šä½å’Œä¸»è¦åŠŸèƒ½
2. æŠ€æœ¯ç‰¹ç‚¹å’Œåˆ›æ–°ä¹‹å¤„
3. é€‚ç”¨åœºæ™¯å’Œç›®æ ‡ç”¨æˆ·
4. é¡¹ç›®ä¼˜åŠ¿å’Œæ½œåœ¨ä¸è¶³
5. å­¦ä¹ å’Œä½¿ç”¨å»ºè®®"""

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¼€æºé¡¹ç›®åˆ†æžå¸ˆï¼Œæ“…é•¿æ·±å…¥åˆ†æžé¡¹ç›®çš„æŠ€æœ¯ç‰¹ç‚¹å’Œåº”ç”¨ä»·å€¼ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.7,
            max_tokens=2000
        )
        
        return response.choices[0].message.content

    def generate_daily_report(self, repos: List[Dict], date_str: str) -> str:
        """
        Generate a complete daily report
        
        Args:
            repos: List of repository dictionaries
            date_str: Date string for the report (e.g., '2026-01-12')
        
        Returns:
            Complete markdown report
        """
        analysis = self.analyze_trends(repos, "comprehensive")
        
        # Build the complete report
        report_parts = [
            f"# ðŸ“ˆ GitHub Trending æ—¥æŠ¥ - {date_str}\n",
            f"> æœ¬æŠ¥å‘Šç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œåˆ†æžäº† GitHub å½“æ—¥ {len(repos)} ä¸ªçƒ­é—¨é¡¹ç›®\n",
            "---\n",
            "## ðŸ“‹ ä»Šæ—¥çƒ­é—¨é¡¹ç›®åˆ—è¡¨\n",
            self._build_repos_summary(repos),
            "\n---\n",
            "## ðŸ¤– AI åˆ†æžæŠ¥å‘Š\n",
            analysis,
            "\n---\n",
            f"*Generated by GitHub Trending Reporter | Data collected at {date_str}*"
        ]
        
        return "\n".join(report_parts)


def analyze_trending(repos: List[Dict], analysis_type: str = "comprehensive") -> str:
    """
    Convenience function to analyze trending repositories
    
    Args:
        repos: List of repository dictionaries
        analysis_type: Type of analysis
    
    Returns:
        Analysis text
    """
    analyzer = LLMAnalyzer()
    return analyzer.analyze_trends(repos, analysis_type)


if __name__ == "__main__":
    # Test with sample data
    sample_repos = [
        {
            "full_name": "test/repo",
            "language": "Python",
            "stars": 1000,
            "stars_today": 100,
            "forks": 50,
            "description": "A test repository",
            "url": "https://github.com/test/repo"
        }
    ]
    
    analyzer = LLMAnalyzer()
    print("Testing LLM Analyzer...")
    # Note: Will fail without valid API key
